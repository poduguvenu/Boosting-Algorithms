# Boosting-Algorithms
1. XGBoost - Customer Churn

  In this notebook we will use XGBoost to build a collection of boosted trees, and use continuous and categorical data from the IBM Base Samples to predict whether or not a customer will stop using a company's service. In business lingo, this is called Customer Churn.

2. XGBoost - Hyperparameter Optimization using RandomizedSearchCV

3. XG Boost, Light GBM, catBoost and Scikit-Learn Gradient Boosting Performance Comparision
  
  In this notebook we'll compare the speed and accuracy of several gradient boosting implementations from Scikit-Learn, XGBoost, LightGBM and CatBoost.

4. Feature Importance and Feature Selection With XGBoost in Python
  
  A benefit of using ensembles of decision tree methods like gradient boosting is that they can automatically provide estimates of feature importance from a trained predictive model.
